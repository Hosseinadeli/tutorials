{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2699171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://s3.amazonaws.com/cornet-models/cornet_s-1d3f7974.pth\" to /home/ha2366/.cache/torch/hub/checkpoints/cornet_s-1d3f7974.pth\n",
      "100%|████████████████████████████████████████| 408M/408M [00:11<00:00, 38.6MB/s]\n",
      "  0%|                                                  | 0/1000 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/engram/nklab/hossein/recurrent_models/CORnet-master/run.py\", line 351, in <module>\n",
      "    fire.Fire(command=FIRE_FLAGS)\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/fire/core.py\", line 141, in Fire\n",
      "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/fire/core.py\", line 475, in _Fire\n",
      "    component, remaining_args = _CallAndUpdateTrace(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
      "    component = fn(*varargs, **kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/engram/nklab/hossein/recurrent_models/CORnet-master/run.py\", line 227, in test\n",
      "    model(im)\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 463, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/conv.py\", line 459, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor\n"
     ]
    }
   ],
   "source": [
    "!python run.py test --model S --data_path '../../nsd/shared1000/' --output_path './test_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "232d5d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(data_path='../../nsd/shared1000/', output_path='./test_results/', model='Z', times=5, ngpus=1, workers=4, epochs=20, batch_size=256, lr=0.1, step_size=10, momentum=0.9, weight_decay=0.0001)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, argparse, time, glob, pickle, subprocess, shlex, io, pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import tqdm\n",
    "import fire\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo\n",
    "import torchvision\n",
    "\n",
    "import cornet\n",
    "\n",
    "from PIL import Image\n",
    "Image.warnings.simplefilter('ignore')\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])\n",
    "\n",
    "def get_args_parser():\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='ImageNet Training', add_help=False)\n",
    "    parser.add_argument('--data_path', required=False,\n",
    "                        help='path to ImageNet folder that contains train and val folders')\n",
    "    parser.add_argument('-o', '--output_path', default=None,\n",
    "                        help='path for storing ')\n",
    "    parser.add_argument('--model', choices=['Z', 'R', 'RT', 'S'], default='Z',\n",
    "                        help='which model to train')\n",
    "    parser.add_argument('--times', default=5, type=int,\n",
    "                        help='number of time steps to run the model (only R model)')\n",
    "    parser.add_argument('--ngpus', default=1, type=int,\n",
    "                        help='number of GPUs to use; 0 if you want to run on CPU')\n",
    "    parser.add_argument('-j', '--workers', default=4, type=int,\n",
    "                        help='number of data loading workers')\n",
    "    parser.add_argument('--epochs', default=20, type=int,\n",
    "                        help='number of total epochs to run')\n",
    "    parser.add_argument('--batch_size', default=256, type=int,\n",
    "                        help='mini-batch size')\n",
    "    parser.add_argument('--lr', '--learning_rate', default=.1, type=float,\n",
    "                        help='initial learning rate')\n",
    "    parser.add_argument('--step_size', default=10, type=int,\n",
    "                        help='after how many epochs learning rate should be decreased 10x')\n",
    "    parser.add_argument('--momentum', default=.9, type=float, help='momentum')\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float,\n",
    "                        help='weight decay ')\n",
    "    return parser\n",
    "\n",
    "parser = argparse.ArgumentParser(description='ImageNet Training', parents=[get_args_parser()])\n",
    "FLAGS = parser.parse_args('')\n",
    "\n",
    "FLAGS.data_path =  '../../nsd/shared1000/'\n",
    "FLAGS.output_path = './test_results/'\n",
    "\n",
    "FLAGS\n",
    "#FLAGS, FIRE_FLAGS = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9cc851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS.model = 'RT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cd216e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def set_gpus(n=1):\n",
    "    \"\"\"\n",
    "    Finds all GPUs on the system and restricts to n of them that have the most\n",
    "    free memory.\n",
    "    \"\"\"\n",
    "    gpus = subprocess.run(shlex.split(\n",
    "        'nvidia-smi --query-gpu=index,memory.free,memory.total --format=csv,nounits'), check=True, stdout=subprocess.PIPE).stdout\n",
    "    gpus = pandas.read_csv(io.BytesIO(gpus), sep=', ', engine='python')\n",
    "    gpus = gpus[gpus['memory.total [MiB]'] > 10000]  # only above 10 GB\n",
    "    if os.environ.get('CUDA_VISIBLE_DEVICES') is not None:\n",
    "        visible = [int(i)\n",
    "                   for i in os.environ['CUDA_VISIBLE_DEVICES'].split(',')]\n",
    "        gpus = gpus[gpus['index'].isin(visible)]\n",
    "    gpus = gpus.sort_values(by='memory.free [MiB]', ascending=False)\n",
    "    os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'  # making sure GPUs are numbered the same way as in nvidia_smi\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(\n",
    "        [str(i) for i in gpus['index'].iloc[:n]])\n",
    "\n",
    "\n",
    "if FLAGS.ngpus > 0:\n",
    "    set_gpus(FLAGS.ngpus)\n",
    "\n",
    "\n",
    "def get_model(pretrained=False):\n",
    "    map_location = None if FLAGS.ngpus > 0 else 'cpu'\n",
    "    model = getattr(cornet, f'cornet_{FLAGS.model.lower()}')\n",
    "    if FLAGS.model.lower() == 'r':\n",
    "        model = model(pretrained=pretrained, map_location=map_location, times=FLAGS.times)\n",
    "    else:\n",
    "        model = model(pretrained=pretrained, map_location=map_location)\n",
    "\n",
    "    if FLAGS.ngpus == 0:\n",
    "        model = model.module  # remove DataParallel\n",
    "    if FLAGS.ngpus > 0:\n",
    "        model = model.cuda()\n",
    "    return model\n",
    "\n",
    "\n",
    "def train(restore_path=None,  # useful when you want to restart training\n",
    "          save_train_epochs=.1,  # how often save output during training\n",
    "          save_val_epochs=.5,  # how often save output during validation\n",
    "          save_model_epochs=5,  # how often save model weigths\n",
    "          save_model_secs=60 * 10  # how often save model (in sec)\n",
    "          ):\n",
    "\n",
    "    model = get_model()\n",
    "    trainer = ImageNetTrain(model)\n",
    "    validator = ImageNetVal(model)\n",
    "\n",
    "    start_epoch = 0\n",
    "    if restore_path is not None:\n",
    "        ckpt_data = torch.load(restore_path)\n",
    "        start_epoch = ckpt_data['epoch']\n",
    "        model.load_state_dict(ckpt_data['state_dict'])\n",
    "        trainer.optimizer.load_state_dict(ckpt_data['optimizer'])\n",
    "\n",
    "    records = []\n",
    "    recent_time = time.time()\n",
    "\n",
    "    nsteps = len(trainer.data_loader)\n",
    "    if save_train_epochs is not None:\n",
    "        save_train_steps = (np.arange(0, FLAGS.epochs + 1,\n",
    "                                      save_train_epochs) * nsteps).astype(int)\n",
    "    if save_val_epochs is not None:\n",
    "        save_val_steps = (np.arange(0, FLAGS.epochs + 1,\n",
    "                                    save_val_epochs) * nsteps).astype(int)\n",
    "    if save_model_epochs is not None:\n",
    "        save_model_steps = (np.arange(0, FLAGS.epochs + 1,\n",
    "                                      save_model_epochs) * nsteps).astype(int)\n",
    "\n",
    "    results = {'meta': {'step_in_epoch': 0,\n",
    "                        'epoch': start_epoch,\n",
    "                        'wall_time': time.time()}\n",
    "               }\n",
    "    for epoch in tqdm.trange(0, FLAGS.epochs + 1, initial=start_epoch, desc='epoch'):\n",
    "        data_load_start = np.nan\n",
    "        for step, data in enumerate(tqdm.tqdm(trainer.data_loader, desc=trainer.name)):\n",
    "            data_load_time = time.time() - data_load_start\n",
    "            global_step = epoch * len(trainer.data_loader) + step\n",
    "\n",
    "            if save_val_steps is not None:\n",
    "                if global_step in save_val_steps:\n",
    "                    results[validator.name] = validator()\n",
    "                    trainer.model.train()\n",
    "\n",
    "            if FLAGS.output_path is not None:\n",
    "                records.append(results)\n",
    "                if len(results) > 1:\n",
    "                    pickle.dump(records, open(os.path.join(FLAGS.output_path, 'results.pkl'), 'wb'))\n",
    "\n",
    "                ckpt_data = {}\n",
    "                ckpt_data['flags'] = FLAGS.__dict__.copy()\n",
    "                ckpt_data['epoch'] = epoch\n",
    "                ckpt_data['state_dict'] = model.state_dict()\n",
    "                ckpt_data['optimizer'] = trainer.optimizer.state_dict()\n",
    "\n",
    "                if save_model_secs is not None:\n",
    "                    if time.time() - recent_time > save_model_secs:\n",
    "                        torch.save(ckpt_data, os.path.join(FLAGS.output_path,\n",
    "                                                           'latest_checkpoint.pth.tar'))\n",
    "                        recent_time = time.time()\n",
    "\n",
    "                if save_model_steps is not None:\n",
    "                    if global_step in save_model_steps:\n",
    "                        torch.save(ckpt_data, os.path.join(FLAGS.output_path,\n",
    "                                                           f'epoch_{epoch:02d}.pth.tar'))\n",
    "\n",
    "            else:\n",
    "                if len(results) > 1:\n",
    "                    pprint.pprint(results)\n",
    "\n",
    "            if epoch < FLAGS.epochs:\n",
    "                frac_epoch = (global_step + 1) / len(trainer.data_loader)\n",
    "                record = trainer(frac_epoch, *data)\n",
    "                record['data_load_dur'] = data_load_time\n",
    "                results = {'meta': {'step_in_epoch': step + 1,\n",
    "                                    'epoch': frac_epoch,\n",
    "                                    'wall_time': time.time()}\n",
    "                           }\n",
    "                if save_train_steps is not None:\n",
    "                    if step in save_train_steps:\n",
    "                        results[trainer.name] = record\n",
    "\n",
    "            data_load_start = time.time()\n",
    "\n",
    "\n",
    "def test(layer='decoder', sublayer='avgpool', time_step=0, imsize=224):\n",
    "    \"\"\"\n",
    "    Suitable for small image sets. If you have thousands of images or it is\n",
    "    taking too long to extract features, consider using\n",
    "    `torchvision.datasets.ImageFolder`, using `ImageNetVal` as an example.\n",
    "\n",
    "    Kwargs:\n",
    "        - layers (choose from: V1, V2, V4, IT, decoder)\n",
    "        - sublayer (e.g., output, conv1, avgpool)\n",
    "        - time_step (which time step to use for storing features)\n",
    "        - imsize (resize image to how many pixels, default: 224)\n",
    "    \"\"\"\n",
    "    model = get_model(pretrained=True)\n",
    "    transform = torchvision.transforms.Compose([\n",
    "                    torchvision.transforms.Resize((imsize, imsize)),\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    normalize,\n",
    "                ])\n",
    "    model.eval()\n",
    "\n",
    "    def _store_feats(layer, inp, output):\n",
    "        \"\"\"An ugly but effective way of accessing intermediate model features\n",
    "        \"\"\"\n",
    "        output = output.cpu().numpy()\n",
    "        _model_feats.append(np.reshape(output, (len(output), -1)))\n",
    "\n",
    "    try:\n",
    "        m = model.module\n",
    "    except:\n",
    "        m = model\n",
    "    model_layer = getattr(getattr(m, layer), sublayer)\n",
    "    model_layer.register_forward_hook(_store_feats)\n",
    "\n",
    "    model_feats = []\n",
    "    with torch.no_grad():\n",
    "        model_feats = []\n",
    "        fnames = sorted(glob.glob(os.path.join(FLAGS.data_path, '*.*')))\n",
    "        if len(fnames) == 0:\n",
    "            raise FileNotFoundError(f'No files found in {FLAGS.data_path}')\n",
    "        for fname in tqdm.tqdm(fnames):\n",
    "            try:\n",
    "                im = Image.open(fname).convert('RGB')\n",
    "            except:\n",
    "                raise FileNotFoundError(f'Unable to load {fname}')\n",
    "            im = transform(im)\n",
    "            im = im.unsqueeze(0)  # adding extra dimension for batch size of 1\n",
    "            _model_feats = []\n",
    "            model(im)\n",
    "            model_feats.append(_model_feats[time_step])\n",
    "        model_feats = np.concatenate(model_feats)\n",
    "\n",
    "    if FLAGS.output_path is not None:\n",
    "        fname = f'CORnet-{FLAGS.model}_{layer}_{sublayer}_feats.npy'\n",
    "        np.save(os.path.join(FLAGS.output_path, fname), model_feats)\n",
    "\n",
    "\n",
    "class ImageNetTrain(object):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.name = 'train'\n",
    "        self.model = model\n",
    "        self.data_loader = self.data()\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(),\n",
    "                                         FLAGS.lr,\n",
    "                                         momentum=FLAGS.momentum,\n",
    "                                         weight_decay=FLAGS.weight_decay)\n",
    "        self.lr = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=FLAGS.step_size)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        if FLAGS.ngpus > 0:\n",
    "            self.loss = self.loss.cuda()\n",
    "\n",
    "    def data(self):\n",
    "        dataset = torchvision.datasets.ImageFolder(\n",
    "            os.path.join(FLAGS.data_path, 'train'),\n",
    "            torchvision.transforms.Compose([\n",
    "                torchvision.transforms.RandomResizedCrop(224),\n",
    "                torchvision.transforms.RandomHorizontalFlip(),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]))\n",
    "        data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                                  batch_size=FLAGS.batch_size,\n",
    "                                                  shuffle=True,\n",
    "                                                  num_workers=FLAGS.workers,\n",
    "                                                  pin_memory=True)\n",
    "        return data_loader\n",
    "\n",
    "    def __call__(self, frac_epoch, inp, target):\n",
    "        start = time.time()\n",
    "\n",
    "        self.lr.step(epoch=frac_epoch)\n",
    "        if FLAGS.ngpus > 0:\n",
    "            target = target.cuda(non_blocking=True)\n",
    "        output = self.model(inp)\n",
    "\n",
    "        record = {}\n",
    "        loss = self.loss(output, target)\n",
    "        record['loss'] = loss.item()\n",
    "        record['top1'], record['top5'] = accuracy(output, target, topk=(1, 5))\n",
    "        record['top1'] /= len(output)\n",
    "        record['top5'] /= len(output)\n",
    "        record['learning_rate'] = self.lr.get_lr()[0]\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        record['dur'] = time.time() - start\n",
    "        return record\n",
    "\n",
    "\n",
    "class ImageNetVal(object):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.name = 'val'\n",
    "        self.model = model\n",
    "        self.data_loader = self.data()\n",
    "        self.loss = nn.CrossEntropyLoss(size_average=False)\n",
    "        if FLAGS.ngpus > 0:\n",
    "            self.loss = self.loss.cuda()\n",
    "\n",
    "    def data(self):\n",
    "        dataset = torchvision.datasets.ImageFolder(\n",
    "            os.path.join(FLAGS.data_path, 'val_in_folders'),\n",
    "            torchvision.transforms.Compose([\n",
    "                torchvision.transforms.Resize(256),\n",
    "                torchvision.transforms.CenterCrop(224),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]))\n",
    "        data_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                                  batch_size=FLAGS.batch_size,\n",
    "                                                  shuffle=False,\n",
    "                                                  num_workers=FLAGS.workers,\n",
    "                                                  pin_memory=True)\n",
    "\n",
    "        return data_loader\n",
    "\n",
    "    def __call__(self):\n",
    "        self.model.eval()\n",
    "        start = time.time()\n",
    "        record = {'loss': 0, 'top1': 0, 'top5': 0}\n",
    "        with torch.no_grad():\n",
    "            for (inp, target) in tqdm.tqdm(self.data_loader, desc=self.name):\n",
    "                if FLAGS.ngpus > 0:\n",
    "                    target = target.cuda(non_blocking=True)\n",
    "                output = self.model(inp)\n",
    "\n",
    "                record['loss'] += self.loss(output, target).item()\n",
    "                p1, p5 = accuracy(output, target, topk=(1, 5))\n",
    "                record['top1'] += p1\n",
    "                record['top5'] += p5\n",
    "\n",
    "        for key in record:\n",
    "            record[key] /= len(self.data_loader.dataset.samples)\n",
    "        record['dur'] = (time.time() - start) / len(self.data_loader)\n",
    "\n",
    "        return record\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        _, pred = output.topk(max(topk), dim=1, largest=True, sorted=True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        res = [correct[:k].sum().item() for k in topk]\n",
    "        return res\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     fire.Fire(command=FIRE_FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc02bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0befb727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:12<00:00, 77.78it/s]\n"
     ]
    }
   ],
   "source": [
    "#def test(layer='decoder', sublayer='avgpool', time_step=0, imsize=224):\n",
    "\n",
    "layer='decoder'\n",
    "sublayer='avgpool' \n",
    "time_step=0 \n",
    "imsize=224\n",
    "\"\"\"\n",
    "Suitable for small image sets. If you have thousands of images or it is\n",
    "taking too long to extract features, consider using\n",
    "`torchvision.datasets.ImageFolder`, using `ImageNetVal` as an example.\n",
    "\n",
    "Kwargs:\n",
    "    - layers (choose from: V1, V2, V4, IT, decoder)\n",
    "    - sublayer (e.g., output, conv1, avgpool)\n",
    "    - time_step (which time step to use for storing features)\n",
    "    - imsize (resize image to how many pixels, default: 224)\n",
    "\"\"\"\n",
    "model = get_model(pretrained=True)\n",
    "transform = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.Resize((imsize, imsize)),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ])\n",
    "model.eval()\n",
    "\n",
    "def _store_feats(layer, inp, output):\n",
    "    \"\"\"An ugly but effective way of accessing intermediate model features\n",
    "    \"\"\"\n",
    "    output = output.cpu().numpy()\n",
    "    _model_feats.append(np.reshape(output, (len(output), -1)))\n",
    "\n",
    "try:\n",
    "    m = model.module\n",
    "except:\n",
    "    m = model\n",
    "model_layer = getattr(getattr(m, layer), sublayer)\n",
    "model_layer.register_forward_hook(_store_feats)\n",
    "\n",
    "model_feats = []\n",
    "with torch.no_grad():\n",
    "    model_feats = []\n",
    "    fnames = sorted(glob.glob(os.path.join(FLAGS.data_path, '*.*')))\n",
    "    if len(fnames) == 0:\n",
    "        raise FileNotFoundError(f'No files found in {FLAGS.data_path}')\n",
    "    for fname in tqdm.tqdm(fnames):\n",
    "        try:\n",
    "            im = Image.open(fname).convert('RGB')\n",
    "        except:\n",
    "            raise FileNotFoundError(f'Unable to load {fname}')\n",
    "        im = transform(im)\n",
    "        im = im.unsqueeze(0)  # adding extra dimension for batch size of 1\n",
    "        _model_feats = []\n",
    "        model(im)\n",
    "        model_feats.append(_model_feats[time_step])\n",
    "    model_feats = np.concatenate(model_feats)\n",
    "\n",
    "if FLAGS.output_path is not None:\n",
    "    fname = f'CORnet-{FLAGS.model}_{layer}_{sublayer}_feats.npy'\n",
    "    np.save(os.path.join(FLAGS.output_path, fname), model_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1318b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): CORnet_RT(\n",
       "    (V1): CORblock_RT(\n",
       "      (conv_input): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "      (norm_input): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "      (nonlin_input): ReLU(inplace=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "      (nonlin1): ReLU(inplace=True)\n",
       "      (output): Identity()\n",
       "    )\n",
       "    (V2): CORblock_RT(\n",
       "      (conv_input): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (norm_input): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "      (nonlin_input): ReLU(inplace=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "      (nonlin1): ReLU(inplace=True)\n",
       "      (output): Identity()\n",
       "    )\n",
       "    (V4): CORblock_RT(\n",
       "      (conv_input): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (norm_input): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      (nonlin_input): ReLU(inplace=True)\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      (nonlin1): ReLU(inplace=True)\n",
       "      (output): Identity()\n",
       "    )\n",
       "    (IT): CORblock_RT(\n",
       "      (conv_input): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (norm_input): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "      (nonlin_input): ReLU(inplace=True)\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (norm1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "      (nonlin1): ReLU(inplace=True)\n",
       "      (output): Identity()\n",
       "    )\n",
       "    (decoder): Sequential(\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "      (flatten): Flatten()\n",
       "      (linear): Linear(in_features=512, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c81efdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(im)\n",
    "    \n",
    "#pred = pred.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8c410946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "799"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(pred.cpu().numpy())  # im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "088054c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bdd9b03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Log of CORnet_RT forward pass:\n",
       "\tRandom seed: 972191058\n",
       "\tTime elapsed: 13.935s (13.91s spent logging)\n",
       "\tStructure:\n",
       "\t\t- recurrent (at most 5 loops)\n",
       "\t\t- with branching\n",
       "\t\t- no conditional (if-then) branching\n",
       "\t\t- 36 total modules\n",
       "\tTensor info:\n",
       "\t\t- 162 total tensors (57.4 MB) computed in forward pass.\n",
       "\t\t- 162 tensors (57.4 MB) with saved activations.\n",
       "\tParameters: 17 parameter operations (5208936 params total; 19.9 MB)\n",
       "\tModule Hierarchy:\n",
       "\t\tV1:1\n",
       "\t\t    V1.conv_input:1, V1.norm_input:1, V1.nonlin_input:1, V1.conv1:1, V1.norm1:1, V1.nonlin1:1, V1.output:1\n",
       "\t\tV2:1\n",
       "\t\t    V2.conv1:1, V2.norm1:1, V2.nonlin1:1, V2.output:1\n",
       "\t\tV4:1\n",
       "\t\t    V4.conv1:1, V4.norm1:1, V4.nonlin1:1, V4.output:1\n",
       "\t\tIT:1\n",
       "\t\t    IT.conv1:1, IT.norm1:1, IT.nonlin1:1, IT.output:1\n",
       "\t\tV1:2\n",
       "\t\t    V1.conv_input:2, V1.norm_input:2, V1.nonlin_input:2, V1.conv1:2, V1.norm1:2, V1.nonlin1:2, V1.output:2\n",
       "\t\tV2:2\n",
       "\t\t    V2.conv_input:1, V2.norm_input:1, V2.nonlin_input:1, V2.conv1:2, V2.norm1:2, V2.nonlin1:2, V2.output:2\n",
       "\t\tV4:2\n",
       "\t\t    V4.conv_input:1, V4.norm_input:1, V4.nonlin_input:1, V4.conv1:2, V4.norm1:2, V4.nonlin1:2, V4.output:2\n",
       "\t\tIT:2\n",
       "\t\t    IT.conv_input:1, IT.norm_input:1, IT.nonlin_input:1, IT.conv1:2, IT.norm1:2, IT.nonlin1:2, IT.output:2\n",
       "\t\tV1:3\n",
       "\t\t    V1.conv_input:3, V1.norm_input:3, V1.nonlin_input:3, V1.conv1:3, V1.norm1:3, V1.nonlin1:3, V1.output:3\n",
       "\t\tV2:3\n",
       "\t\t    V2.conv_input:2, V2.norm_input:2, V2.nonlin_input:2, V2.conv1:3, V2.norm1:3, V2.nonlin1:3, V2.output:3\n",
       "\t\tV4:3\n",
       "\t\t    V4.conv_input:2, V4.norm_input:2, V4.nonlin_input:2, V4.conv1:3, V4.norm1:3, V4.nonlin1:3, V4.output:3\n",
       "\t\tIT:3\n",
       "\t\t    IT.conv_input:2, IT.norm_input:2, IT.nonlin_input:2, IT.conv1:3, IT.norm1:3, IT.nonlin1:3, IT.output:3\n",
       "\t\tV1:4\n",
       "\t\t    V1.conv_input:4, V1.norm_input:4, V1.nonlin_input:4, V1.conv1:4, V1.norm1:4, V1.nonlin1:4, V1.output:4\n",
       "\t\tV2:4\n",
       "\t\t    V2.conv_input:3, V2.norm_input:3, V2.nonlin_input:3, V2.conv1:4, V2.norm1:4, V2.nonlin1:4, V2.output:4\n",
       "\t\tV4:4\n",
       "\t\t    V4.conv_input:3, V4.norm_input:3, V4.nonlin_input:3, V4.conv1:4, V4.norm1:4, V4.nonlin1:4, V4.output:4\n",
       "\t\tIT:4\n",
       "\t\t    IT.conv_input:3, IT.norm_input:3, IT.nonlin_input:3, IT.conv1:4, IT.norm1:4, IT.nonlin1:4, IT.output:4\n",
       "\t\tV1:5\n",
       "\t\t    V1.conv_input:5, V1.norm_input:5, V1.nonlin_input:5, V1.conv1:5, V1.norm1:5, V1.nonlin1:5, V1.output:5\n",
       "\t\tV2:5\n",
       "\t\t    V2.conv_input:4, V2.norm_input:4, V2.nonlin_input:4, V2.conv1:5, V2.norm1:5, V2.nonlin1:5, V2.output:5\n",
       "\t\tV4:5\n",
       "\t\t    V4.conv_input:4, V4.norm_input:4, V4.nonlin_input:4, V4.conv1:5, V4.norm1:5, V4.nonlin1:5, V4.output:5\n",
       "\t\tIT:5\n",
       "\t\t    IT.conv_input:4, IT.norm_input:4, IT.nonlin_input:4, IT.conv1:5, IT.norm1:5, IT.nonlin1:5, IT.output:5\n",
       "\t\tdecoder\n",
       "\t\t    decoder.avgpool, decoder.flatten, decoder.linear\n",
       "\tLayers (all have saved activations):\n",
       "\t\t  (0) input_1 \n",
       "\t\t  (1) conv2d_1_1:1  (1/5 passes)\n",
       "\t\t  (2) groupnorm_1_2:1  (1/5 passes)\n",
       "\t\t  (3) relu_1_3:1  (1/5 passes)\n",
       "\t\t  (4) add_1_4 \n",
       "\t\t  (5) conv2d_2_5:1  (1/5 passes)\n",
       "\t\t  (6) groupnorm_2_6:1  (1/5 passes)\n",
       "\t\t  (7) relu_2_7:1  (1/5 passes)\n",
       "\t\t  (8) identity_1_8:1  (1/5 passes)\n",
       "\t\t  (9) zeros_1_9 \n",
       "\t\t  (10) cuda_1_10 \n",
       "\t\t  (11) add_2_11 \n",
       "\t\t  (12) conv2d_3_12:1  (1/5 passes)\n",
       "\t\t  (13) groupnorm_3_13:1  (1/5 passes)\n",
       "\t\t  (14) relu_3_14:1  (1/5 passes)\n",
       "\t\t  (15) identity_2_15:1  (1/5 passes)\n",
       "\t\t  (16) zeros_2_16 \n",
       "\t\t  (17) cuda_2_17 \n",
       "\t\t  (18) add_3_18 \n",
       "\t\t  (19) conv2d_4_19:1  (1/5 passes)\n",
       "\t\t  (20) groupnorm_4_20:1  (1/5 passes)\n",
       "\t\t  (21) relu_4_21:1  (1/5 passes)\n",
       "\t\t  (22) identity_3_22:1  (1/5 passes)\n",
       "\t\t  (23) zeros_3_23 \n",
       "\t\t  (24) cuda_3_24 \n",
       "\t\t  (25) add_4_25 \n",
       "\t\t  (26) conv2d_5_26:1  (1/5 passes)\n",
       "\t\t  (27) groupnorm_5_27:1  (1/5 passes)\n",
       "\t\t  (28) relu_5_28:1  (1/5 passes)\n",
       "\t\t  (29) identity_4_29:1  (1/5 passes)\n",
       "\t\t  (30) conv2d_1_1:2  (2/5 passes)\n",
       "\t\t  (31) groupnorm_1_2:2  (2/5 passes)\n",
       "\t\t  (32) relu_1_3:2  (2/5 passes)\n",
       "\t\t  (33) add_5_30:1  (1/4 passes)\n",
       "\t\t  (34) conv2d_2_5:2  (2/5 passes)\n",
       "\t\t  (35) groupnorm_2_6:2  (2/5 passes)\n",
       "\t\t  (36) relu_2_7:2  (2/5 passes)\n",
       "\t\t  (37) identity_1_8:2  (2/5 passes)\n",
       "\t\t  (38) conv2d_6_31:1  (1/4 passes)\n",
       "\t\t  (39) groupnorm_6_32:1  (1/4 passes)\n",
       "\t\t  (40) relu_6_33:1  (1/4 passes)\n",
       "\t\t  (41) add_6_34:1  (1/4 passes)\n",
       "\t\t  (42) conv2d_3_12:2  (2/5 passes)\n",
       "\t\t  (43) groupnorm_3_13:2  (2/5 passes)\n",
       "\t\t  (44) relu_3_14:2  (2/5 passes)\n",
       "\t\t  (45) identity_2_15:2  (2/5 passes)\n",
       "\t\t  (46) conv2d_7_35:1  (1/4 passes)\n",
       "\t\t  (47) groupnorm_7_36:1  (1/4 passes)\n",
       "\t\t  (48) relu_7_37:1  (1/4 passes)\n",
       "\t\t  (49) add_7_38:1  (1/4 passes)\n",
       "\t\t  (50) conv2d_4_19:2  (2/5 passes)\n",
       "\t\t  (51) groupnorm_4_20:2  (2/5 passes)\n",
       "\t\t  (52) relu_4_21:2  (2/5 passes)\n",
       "\t\t  (53) identity_3_22:2  (2/5 passes)\n",
       "\t\t  (54) conv2d_8_39:1  (1/4 passes)\n",
       "\t\t  (55) groupnorm_8_40:1  (1/4 passes)\n",
       "\t\t  (56) relu_8_41:1  (1/4 passes)\n",
       "\t\t  (57) add_8_42:1  (1/4 passes)\n",
       "\t\t  (58) conv2d_5_26:2  (2/5 passes)\n",
       "\t\t  (59) groupnorm_5_27:2  (2/5 passes)\n",
       "\t\t  (60) relu_5_28:2  (2/5 passes)\n",
       "\t\t  (61) identity_4_29:2  (2/5 passes)\n",
       "\t\t  (62) conv2d_1_1:3  (3/5 passes)\n",
       "\t\t  (63) groupnorm_1_2:3  (3/5 passes)\n",
       "\t\t  (64) relu_1_3:3  (3/5 passes)\n",
       "\t\t  (65) add_5_30:2  (2/4 passes)\n",
       "\t\t  (66) conv2d_2_5:3  (3/5 passes)\n",
       "\t\t  (67) groupnorm_2_6:3  (3/5 passes)\n",
       "\t\t  (68) relu_2_7:3  (3/5 passes)\n",
       "\t\t  (69) identity_1_8:3  (3/5 passes)\n",
       "\t\t  (70) conv2d_6_31:2  (2/4 passes)\n",
       "\t\t  (71) groupnorm_6_32:2  (2/4 passes)\n",
       "\t\t  (72) relu_6_33:2  (2/4 passes)\n",
       "\t\t  (73) add_6_34:2  (2/4 passes)\n",
       "\t\t  (74) conv2d_3_12:3  (3/5 passes)\n",
       "\t\t  (75) groupnorm_3_13:3  (3/5 passes)\n",
       "\t\t  (76) relu_3_14:3  (3/5 passes)\n",
       "\t\t  (77) identity_2_15:3  (3/5 passes)\n",
       "\t\t  (78) conv2d_7_35:2  (2/4 passes)\n",
       "\t\t  (79) groupnorm_7_36:2  (2/4 passes)\n",
       "\t\t  (80) relu_7_37:2  (2/4 passes)\n",
       "\t\t  (81) add_7_38:2  (2/4 passes)\n",
       "\t\t  (82) conv2d_4_19:3  (3/5 passes)\n",
       "\t\t  (83) groupnorm_4_20:3  (3/5 passes)\n",
       "\t\t  (84) relu_4_21:3  (3/5 passes)\n",
       "\t\t  (85) identity_3_22:3  (3/5 passes)\n",
       "\t\t  (86) conv2d_8_39:2  (2/4 passes)\n",
       "\t\t  (87) groupnorm_8_40:2  (2/4 passes)\n",
       "\t\t  (88) relu_8_41:2  (2/4 passes)\n",
       "\t\t  (89) add_8_42:2  (2/4 passes)\n",
       "\t\t  (90) conv2d_5_26:3  (3/5 passes)\n",
       "\t\t  (91) groupnorm_5_27:3  (3/5 passes)\n",
       "\t\t  (92) relu_5_28:3  (3/5 passes)\n",
       "\t\t  (93) identity_4_29:3  (3/5 passes)\n",
       "\t\t  (94) conv2d_1_1:4  (4/5 passes)\n",
       "\t\t  (95) groupnorm_1_2:4  (4/5 passes)\n",
       "\t\t  (96) relu_1_3:4  (4/5 passes)\n",
       "\t\t  (97) add_5_30:3  (3/4 passes)\n",
       "\t\t  (98) conv2d_2_5:4  (4/5 passes)\n",
       "\t\t  (99) groupnorm_2_6:4  (4/5 passes)\n",
       "\t\t  (100) relu_2_7:4  (4/5 passes)\n",
       "\t\t  (101) identity_1_8:4  (4/5 passes)\n",
       "\t\t  (102) conv2d_6_31:3  (3/4 passes)\n",
       "\t\t  (103) groupnorm_6_32:3  (3/4 passes)\n",
       "\t\t  (104) relu_6_33:3  (3/4 passes)\n",
       "\t\t  (105) add_6_34:3  (3/4 passes)\n",
       "\t\t  (106) conv2d_3_12:4  (4/5 passes)\n",
       "\t\t  (107) groupnorm_3_13:4  (4/5 passes)\n",
       "\t\t  (108) relu_3_14:4  (4/5 passes)\n",
       "\t\t  (109) identity_2_15:4  (4/5 passes)\n",
       "\t\t  (110) conv2d_7_35:3  (3/4 passes)\n",
       "\t\t  (111) groupnorm_7_36:3  (3/4 passes)\n",
       "\t\t  (112) relu_7_37:3  (3/4 passes)\n",
       "\t\t  (113) add_7_38:3  (3/4 passes)\n",
       "\t\t  (114) conv2d_4_19:4  (4/5 passes)\n",
       "\t\t  (115) groupnorm_4_20:4  (4/5 passes)\n",
       "\t\t  (116) relu_4_21:4  (4/5 passes)\n",
       "\t\t  (117) identity_3_22:4  (4/5 passes)\n",
       "\t\t  (118) conv2d_8_39:3  (3/4 passes)\n",
       "\t\t  (119) groupnorm_8_40:3  (3/4 passes)\n",
       "\t\t  (120) relu_8_41:3  (3/4 passes)\n",
       "\t\t  (121) add_8_42:3  (3/4 passes)\n",
       "\t\t  (122) conv2d_5_26:4  (4/5 passes)\n",
       "\t\t  (123) groupnorm_5_27:4  (4/5 passes)\n",
       "\t\t  (124) relu_5_28:4  (4/5 passes)\n",
       "\t\t  (125) identity_4_29:4  (4/5 passes)\n",
       "\t\t  (126) conv2d_1_1:5  (5/5 passes)\n",
       "\t\t  (127) groupnorm_1_2:5  (5/5 passes)\n",
       "\t\t  (128) relu_1_3:5  (5/5 passes)\n",
       "\t\t  (129) add_5_30:4  (4/4 passes)\n",
       "\t\t  (130) conv2d_2_5:5  (5/5 passes)\n",
       "\t\t  (131) groupnorm_2_6:5  (5/5 passes)\n",
       "\t\t  (132) relu_2_7:5  (5/5 passes)\n",
       "\t\t  (133) identity_1_8:5  (5/5 passes)\n",
       "\t\t  (134) conv2d_6_31:4  (4/4 passes)\n",
       "\t\t  (135) groupnorm_6_32:4  (4/4 passes)\n",
       "\t\t  (136) relu_6_33:4  (4/4 passes)\n",
       "\t\t  (137) add_6_34:4  (4/4 passes)\n",
       "\t\t  (138) conv2d_3_12:5  (5/5 passes)\n",
       "\t\t  (139) groupnorm_3_13:5  (5/5 passes)\n",
       "\t\t  (140) relu_3_14:5  (5/5 passes)\n",
       "\t\t  (141) identity_2_15:5  (5/5 passes)\n",
       "\t\t  (142) conv2d_7_35:4  (4/4 passes)\n",
       "\t\t  (143) groupnorm_7_36:4  (4/4 passes)\n",
       "\t\t  (144) relu_7_37:4  (4/4 passes)\n",
       "\t\t  (145) add_7_38:4  (4/4 passes)\n",
       "\t\t  (146) conv2d_4_19:5  (5/5 passes)\n",
       "\t\t  (147) groupnorm_4_20:5  (5/5 passes)\n",
       "\t\t  (148) relu_4_21:5  (5/5 passes)\n",
       "\t\t  (149) identity_3_22:5  (5/5 passes)\n",
       "\t\t  (150) conv2d_8_39:4  (4/4 passes)\n",
       "\t\t  (151) groupnorm_8_40:4  (4/4 passes)\n",
       "\t\t  (152) relu_8_41:4  (4/4 passes)\n",
       "\t\t  (153) add_8_42:4  (4/4 passes)\n",
       "\t\t  (154) conv2d_5_26:5  (5/5 passes)\n",
       "\t\t  (155) groupnorm_5_27:5  (5/5 passes)\n",
       "\t\t  (156) relu_5_28:5  (5/5 passes)\n",
       "\t\t  (157) identity_4_29:5  (5/5 passes)\n",
       "\t\t  (158) adaptiveavgpool2d_1_43 \n",
       "\t\t  (159) view_1_44 \n",
       "\t\t  (160) linear_1_45 \n",
       "\t\t  (161) output_1 "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f841c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
